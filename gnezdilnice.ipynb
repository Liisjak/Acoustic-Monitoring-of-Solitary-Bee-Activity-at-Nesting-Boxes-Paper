{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sklearn\n",
    "from sklearn import mixture\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import time\n",
    "import timedelta\n",
    "import sys\n",
    "import random\n",
    "import cv2\n",
    "import importlib\n",
    "import argparse\n",
    "import glob\n",
    "\n",
    "EOL = os.path.join('') # Change accordingly\n",
    "sys.path.append(EOL)\n",
    "\n",
    "import gnezdilnice_train\n",
    "import gnezdilnice_plotters\n",
    "import gnezdilnice_dataloader\n",
    "import gnezdilnice_models\n",
    "import gnezdilnice_utils\n",
    "\n",
    "# change the width of the cells\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNEZDILNICE = os.path.join(EOL)\n",
    "DATA = gnezdilnice_utils.check_folder(os.path.join(GNEZDILNICE, 'data'))\n",
    "RESULTS = gnezdilnice_utils.check_folder(os.path.join(GNEZDILNICE, 'results'))\n",
    "OUTPUTS = gnezdilnice_utils.check_folder(os.path.join(GNEZDILNICE, 'outputs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Training')\n",
    "parser.add_argument('--dataset', default='gnezdilnice_spectrograms_50-1450_128', type=str)\n",
    "parser.add_argument('--model', default='resnet9', type=str)\n",
    "parser.add_argument('--method', default='base', type=str)\n",
    "parser.add_argument('--split', default='cv', type=str, help='type of experiment (cross-validation, loso)')\n",
    "parser.add_argument('--faint', default=False, type=str, help='are faint buzzes included')\n",
    "parser.add_argument('--high', default=False, type=str, help='are high buzzes included')\n",
    "parser.add_argument('--seed_data', default=3, type=int, help='dataset seed when selecting folds')\n",
    "parser.add_argument('--num_epochs', default=100, type=int)\n",
    "parser.add_argument('--num_steps', default=100, type=int)\n",
    "parser.add_argument('--batch_size', default=128, type=int, help='train batchsize')\n",
    "parser.add_argument('--lr_max', default=0.01, type=float, help='maximum allowed learning rate')\n",
    "parser.add_argument('--use_sched', default=True, type=bool, help='whether to use learning rate scheduler')\n",
    "parser.add_argument('--weight_decay', default=1e-4, type=float, help='weight decay (L2 penalty)')\n",
    "parser.add_argument('--grad_clip', default=0.1, type=float, help='gradient clipping to prevent exploding gradients')\n",
    "parser.add_argument('--depth', default=0, type=int)\n",
    "parser.add_argument('--DATA', default = DATA, type=str, help='path to data')\n",
    "parser.add_argument('--RESULTS', default = RESULTS, type=str, help='path to results')\n",
    "parser.add_argument('--seed_fix', default = 4, type=int, help='seed to fix the numpy, random, torch, and os random seeds')\n",
    "parser.add_argument('-f') # dummy argument to prevent an error, since argparse is a module designed to parse the arguments passed from the command line\n",
    "args = parser.parse_args()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Device set to: cuda')\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "    print('Device set to: cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "args.dataset = 'gnezdilnice_spectrograms_50-1450_128'\n",
    "DATASET = os.path.join(DATA, f'{args.dataset}.dat')\n",
    "dataset = gnezdilnice_utils.file2dict(DATASET)\n",
    "print(f'Dataset {DATASET} has been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args, device):\n",
    "    RESULTS_ARGS = gnezdilnice_utils.results_dir(args)\n",
    "    MODEL = os.path.join(RESULTS_ARGS, 'model.pth')\n",
    "    model = gnezdilnice_models.ResNet9(num_classes = 2)\n",
    "    model = nn.DataParallel(model) # sets to all available cuda devices\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(MODEL))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_buzz_probabilities_rec(data, seed_datas, device):\n",
    "    # find minimum of whole dataset\n",
    "    data_min = -1.891314\n",
    "    data_rec  = torch.tensor(data)\n",
    "    # initialize empty data_rec\n",
    "    data_rec_halves = torch.ones((data_rec.shape[0]+1, data_rec.shape[1], data_rec.shape[2]))*data_min\n",
    "    for drh, d in zip(data_rec_halves, data_rec):\n",
    "        drh[:, :64] = d[:, :64]\n",
    "    data_rec_halves[-1][:, 64:] = data_rec[-1][:, 64:]\n",
    "    data_rec_halves = data_rec_halves[:, None, :, :]\n",
    "    #print(f'{data_rec_halves.shape=}')\n",
    "    # Check models trained on the whole data (different data shuffle)\n",
    "    pred_proba_persec_allseeds = []\n",
    "    for seed_data in seed_datas:\n",
    "        args.seed_data = seed_data\n",
    "        # load the model\n",
    "        model = load_model(args, device)\n",
    "        # iterate over data in smaller chunkes not to overflow the memory\n",
    "        batch = 64\n",
    "        start_index = 0\n",
    "        end_index = batch\n",
    "        pred_proba_seed = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(int(np.ceil(data_rec_halves.shape[0]/batch))):\n",
    "                #print(start_index, end_index)\n",
    "                data_rec_batch = data_rec_halves[start_index:end_index].to(device)\n",
    "                output = model(data_rec_batch)\n",
    "                pred_proba = F.softmax(output, dim=1).cpu().detach().numpy()[:, 1] # buzz prediction probability\n",
    "#                 for d, p in zip(data_rec_batch, pred_proba):\n",
    "#                     plt.imshow(d.detach().cpu().numpy()[0])\n",
    "#                     plt.title(p)\n",
    "#                     plt.show()\n",
    "#                     plt.close()\n",
    "                pred_proba_seed = pred_proba_seed + list(pred_proba)\n",
    "                start_index = start_index + batch\n",
    "                end_index = end_index + batch\n",
    "                if end_index > data_rec.shape[0]:\n",
    "                    end_index = data_rec.shape[0]\n",
    "        pred_proba_persec_seed = [x for x in pred_proba_seed for _ in (0, 1)]\n",
    "        pred_proba_persec_allseeds.append(pred_proba_persec_seed)\n",
    "    mean_array = np.mean(pred_proba_persec_allseeds, axis=0)\n",
    "    std_array = np.std(pred_proba_persec_allseeds, axis=0)\n",
    "    return mean_array, std_array, pred_proba_persec_allseeds\n",
    "\n",
    "def plot_buzz_probablity(pp_mean, pp_std, RECORDING, args, save=True):\n",
    "    color = 'orange'\n",
    "    plt.figure(figsize=(25, 2))\n",
    "    upper = [min(1, x+sd) for x, sd in zip(pp_mean, pp_std)]\n",
    "    lower = [max(0, x-sd) for x, sd in zip(pp_mean, pp_std)]\n",
    "    times = np.arange(0.5, len(pp_mean)+0.5, 1)\n",
    "    plt.plot(pp_mean, color=color )\n",
    "    plt.fill_between(times, lower, upper, alpha=0.25, color=color)\n",
    "    plt.xlim(left=0, right=len(pp_mean)+1)\n",
    "    plt.title(RECORDING)\n",
    "    plt.ylabel('Buzz probability')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.xticks(np.arange(0, len(pp_mean), 60*5))\n",
    "    FN = os.path.join(OUTPUTS, f'buzzproba_{args.dataset}_{args.method}_fnt={args.faint}_high={args.high}_epochs={args.num_epochs}_bs={args.batch_size}_lrmax={args.lr_max}_{RECORDING}.pdf')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(FN, dpi=600)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return\n",
    "\n",
    "def predict_labels(data, seed_datas, device):\n",
    "    # find minimum of whole dataset\n",
    "    data_min = -1.891314\n",
    "    data_rec = torch.tensor(data)\n",
    "    data_rec = data_rec[:, None, :, :]\n",
    "    pred_proba_persec_allseeds = []\n",
    "    for seed_data in seed_datas:\n",
    "        args.seed_data = seed_data\n",
    "        # load the model\n",
    "        model = load_model(args, device)\n",
    "        # iterate over data in smaller chunkes not to overflow the memory\n",
    "        batch = 64\n",
    "        start_index = 0\n",
    "        end_index = batch\n",
    "        pred_proba_seed = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(int(np.ceil(data_rec.shape[0]/batch))):\n",
    "                data_rec_batch = data_rec[start_index:end_index].to(device)\n",
    "                output = model(data_rec_batch)\n",
    "                pred_proba = F.softmax(output, dim=1).cpu().detach().numpy()[:, 1] # buzz prediction probability\n",
    "                pred_proba_seed = pred_proba_seed + list(pred_proba)\n",
    "                start_index = start_index + batch\n",
    "                end_index = end_index + batch\n",
    "                if end_index > data_rec.shape[0]:\n",
    "                    end_index = data_rec.shape[0]\n",
    "        pred_proba_persec_allseeds.append(pred_proba_seed)\n",
    "    mean_array = np.mean(pred_proba_persec_allseeds, axis=0)\n",
    "    std_array = np.std(pred_proba_persec_allseeds, axis=0)\n",
    "    pred_array = [0 if value < 0.5 else 1 for value in mean_array]\n",
    "    return mean_array, std_array, pred_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.split='cv'\n",
    "\n",
    "args.batch_size = 64\n",
    "args.num_epochs = 10\n",
    "args.lr_max = 0.001\n",
    "args.faint = False\n",
    "args.high = False\n",
    "\n",
    "args.method = 'base'\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    args.seed_data = seed\n",
    "    if gnezdilnice_utils.experiment_already_done(args):\n",
    "        print('Already done:', gnezdilnice_utils.results_dir(args))\n",
    "        continue\n",
    "    gnezdilnice_train.train_model(args, dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Location-Out Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args.split='loc'\n",
    "\n",
    "args.batch_size = 64\n",
    "args.num_epochs = 10\n",
    "args.lr_max = 0.001\n",
    "args.faint = False\n",
    "args.high = False\n",
    "\n",
    "for seed in [1, 2, 3]:\n",
    "    args.seed_data = seed\n",
    "    if gnezdilnice_utils.experiment_already_done(args):\n",
    "        print('Already done:', gnezdilnice_utils.results_dir(args))\n",
    "        continue\n",
    "    gnezdilnice_train.train_model(args, dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select hyperparameters based on the CV experiments\n",
    "args.model = 'resnet9'\n",
    "args.split='alltrain'\n",
    "\n",
    "args.batch_size = 64\n",
    "args.num_epochs = 10\n",
    "args.lr_max = 0.001\n",
    "args.faint = False\n",
    "args.high = False\n",
    "\n",
    "args.method = 'base'\n",
    "for seed_data in [1, 2, 3, 4, 5]:\n",
    "    args.seed_data = seed_data\n",
    "    gnezdilnice_train.train_model(args, dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: make predictions on the validation data using a model trained on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained models args\n",
    "args.dataset = f'gnezdilnice_spectrograms_50-1450_128'\n",
    "args.model = 'resnet9'\n",
    "args.split='alltrain'\n",
    "args.batch_size = 64\n",
    "args.num_epochs = 10\n",
    "args.lr_max = 0.001\n",
    "args.faint = False\n",
    "args.high = False\n",
    "args.method = 'base'\n",
    "seed_datas = [1, 2, 3, 4, 5]\n",
    "\n",
    "VALID_DATAS = [\n",
    "    'gnezdilnice_validation-20240314140023-001_crop_spectrograms_50-1450_128.dat',\n",
    "    'gnezdilnice_validation-20240314145532-002_crop_spectrograms_50-1450_128.dat',\n",
    "       ]\n",
    "\n",
    "for VALID_DATA in VALID_DATAS:\n",
    "    DICT =  os.path.join(OUTPUTS, f'output_{VALID_DATA.split(\".dat\")[0]}.pkl')\n",
    "    print(f'Getting output for: {VALID_DATA.split(\".dat\")[0]}')\n",
    "    DATASET = os.path.join(DATA, VALID_DATA)\n",
    "    outputs = {}\n",
    "    print(f'\\t{DATASET}')\n",
    "    dataset = gnezdilnice_utils.file2dict(DATASET)\n",
    "    data = np.array(dataset['data'])\n",
    "    location_rec = np.array(dataset['location'], dtype='str')[0]\n",
    "    date_rec = np.array(dataset['datetime'], dtype='str')[0].split(' ')[0]\n",
    "    time_rec = np.array(dataset['datetime'], dtype='str')[0].split(' ')[1]\n",
    "    # get output\n",
    "    pp_mean, pp_std, _ = predict_labels(data, seed_datas, device)\n",
    "    # plot\n",
    "    plot_buzz_probablity(pp_mean, pp_std, VALID_DATA.split(\".dat\")[0], args, save=False)\n",
    "    # write to dictionary\n",
    "    outputs_date = {\n",
    "                   'location':location_rec, \n",
    "                   'time':time_rec,\n",
    "                   'buzz_mean': pp_mean,\n",
    "                   'buzz_std': pp_std,\n",
    "                    }\n",
    "    outputs[date_rec] = outputs_date\n",
    "    gnezdilnice_utils.save_dict(outputs, DICT)\n",
    "    print(f'Saved outputs to: {DICT}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
